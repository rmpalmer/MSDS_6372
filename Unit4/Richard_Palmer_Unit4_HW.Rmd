---
title: "Unit4 Homework"
author: "Richard Palmer"
date: "9/17/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Conceptual Questions

### For a time series to be stationary

- Constant Mean
- Constant Variance
- Constant Autocorrelations

### if a timee series model ..
includes and explantory variable such as time itself, or some other predictor,
then the original time series of the response is not stationary.
(FALSE) - this could *hide* nonstationarity, but would not mean it is necessarily present.

### The main drawback of serially correlated observations is..

That there is less information from which to create a model.

### The main advantage of serrially correlated observations is..

that it is possible to make predictions about the future values of the time series.

### The purpose of the Durbin Watson test is...

to detect the presence of serial correlation in a time series.

## Exercise 1

6.  Verity rules of thumb

```{r}
AR1 <- arima.sim(list(ar=c(0.8)),10000)
par(mfrow=c(1,3))
plot(1:10000,AR1,type="l")
ar1_acf <- acf(AR1,main="ACF")
ar1_pacf <-pacf(AR1,main="PACF")
```

The acf clearly decays gradually.  The lag1 autocorrelation is `r unlist(ar1_acf)[2]`.

7.  Repeat 3 times with 50 observations each

```{r}
AR1_short = arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:50,AR1_short,type="l")
AR1_short_acf <- acf(AR1_short,main="ACF")
AR1_short_pacf <-pacf(AR1_short,main="PACF")
```

The lag1 autocorrelation is `r unlist(AR1_short_acf)[2]`.

```{r}
AR1_short = arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:50,AR1_short,type="l")
AR1_short_acf <- acf(AR1_short,main="ACF")
AR1_short_pacf <-pacf(AR1_short,main="PACF")
```

The lag1 autocorrelation is `r unlist(AR1_short_acf)[2]`.

```{r}
AR1_short = arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:50,AR1_short,type="l")
AR1_short_acf <- acf(AR1_short,main="ACF")
AR1_short_pacf <-pacf(AR1_short,main="PACF")
```

The lag1 autocorrelation is `r unlist(AR1_short_acf)[2]`.

8.  Simulate with autocorrlelation set to 0

```{r}
AR1 <- arima.sim(list(ar=c(0.0)),10000)
par(mfrow=c(1,3))
plot(1:10000,AR1,type="l")
ar1_acf <- acf(AR1,main="ACF")
ar1_pacf <-pacf(AR1,main="PACF")
```

## Exercise 2

9.  Differing data generation

_AR2 Behavior_
```{r echo=T}
rho1<-.8
rho2<-.6
a1<-(rho1*(1-rho2)/(1-rho1^2))
a2<-(rho2-rho1^2)/(1-rho1^2)
AR2<-arima.sim(list(ar=c(a1,a2)),10000)
par(mfrow=c(1,3))
plot(1:10000,AR2,type="l")
acf(AR2)
pacf(AR2,main="PACF")

```

For AR2, expect ACF to decay asymptotically, and expect PACF to terminate after 2 lags.  They do.

_AR3 Behavior_
```{r echo=T}

a1<-1.5
a2<--1.21
a3<-.46
AR3<-arima.sim(list(ar=c(a1,a2,a3)),10000)
par(mfrow=c(1,3))
plot(1:10000,AR3,type="l")
acf(AR3,main="ACF")
pacf(AR3,main="PACF")

```

For AR3, expect ACF to decay asymptotically, and expect PACF to terminate after 3 lags.  They do.

_ARMA(3,2) Behavior_
```{r echo=T}

a1<-1.5
a2<--1.21
a3<-.46
b1<--.2
b2<--.9
ARMA32<-arima.sim(list(ar=c(a1,a2,a3),ma=c(b1,b2)),10000)
par(mfrow=c(1,3))
plot(1:10000,ARMA32,type="l")
acf(ARMA32,main="ACF")
pacf(ARMA32,main="PACF")

```
For an ARMA(3,2) series, expect ACF and PACF to decay exponentially, and they do.

_Moving Average MA(2) Behavior_
```{r  echo=T}

b1<- .2
b2<- .9
MA2<-arima.sim(list(ma=c(b1,b2)),10000)
par(mfrow=c(1,3))
plot(1:10000,MA2,type="l")
acf(MA2,main="ACF")
pacf(MA2,main="PACF")

```
For a moving average, expect ACF to cut off after 2 lags, and it does
(the three lags with significant values are 0, 1, and 2)
Expect PACF to decay asymptotically, and it does.

## Exercise 3

```{r, echo=T}
# load the data
library(tseries)
library(forecast)
library(ggplot2)
bills<-read.csv("ElectricBill.csv")
bills$DateIndex<-1:nrow(bills)
```

Original series
```{r}
attach(bills)
Acf(Bill)
Pacf(Bill)
```

10.  Compare diagnostics from models AR1 through AR5

```{r}
AR1 <- arima(Bill,order=c(1,0,0))
tsdisplay(residuals(AR1),lag.max=15,main="AR(1) residual diagnostics")
```

AR1 residuals, ACF, and PACF coefficients clearly still have a pattern.

```{r}
AR2 <- arima(Bill,order=c(2,0,0))
tsdisplay(residuals(AR2),lag.max=15,main="AR(2) residual diagnostics")
```
 
AR2 residuals look more random, but ACF coefficients are still not small.

```{r}
AR3 <- arima(Bill,order=c(3,0,0))
tsdisplay(residuals(AR3),lag.max=15,main="AR(3) residual diagnostics")
```

From AR2 to AR3, there is not much change.

```{r}
AR4 <- arima(Bill,order=c(4,0,0))
tsdisplay(residuals(AR4),lag.max=15,main="AR(4) residual diagnostics")
```

With AR4, the ACF and PACF coefficients are now smaller.

```{r}
AR5 <- arima(Bill,order=c(5,0,0))
tsdisplay(residuals(AR5),lag.max=15,main="AR(5) residual diagnostics")
```

From AR4 to AR5 there is again not much change.

11.  check AIC from each of the models.

```{r}
#AIC from AR1 is 
AIC(AR1)
#AIC from AR2 is 
AIC(AR2)
#AIC from AR3 is
AIC(AR3)
#AIC from AR4 is 
AIC(AR4)
#AIC from AR5 is
AIC(AR5)
```

Indeed, the AR4 model, which has the lowest AIC, does appear to have good residual diagnostics.


12.  Compare forecast of AR1 with AR4

```{r}
AR1.fit <- auto.arima(Bill,seasonal=FALSE,stepwise = FALSE,max.p=1,start.p=1,max.d=0,max.q=0)
plot(forecast(AR1.fit,h=10))
```


```{r}
AR4.fit <- auto.arima(Bill,seasonal=FALSE,stepwise = FALSE,max.p=4,start.p=4,max.d=0,max.q=0)
plot(forecast(AR4.fit,h=10))
```

The AR4 model clearly provides a more realistic prediction than AR1.  The AR1 model is essentially
quickly decaying to the mean, with widening error bars.  The AR4 model predicts the next cycle in
a believable way, though its error bars are still widening slightly with increasing time.

13.  Forecast out to 100 observations in the future.

```{r}
plot(forecast(AR4.fit,h=100))
```

The forecast must fall back to the mean for a stationary series.  This does occur.  I 
expect the standard error bars to asymptotically approach the standard deviation of the
original series, without regard to a specific time sample.  This is because for predicting
observations 100 steps into the future, there is not going to be a significant difference
between time 99 and time 100.  Both points in time are very nearly the same distance away
from where there is recorded data, so the model could not be expected to distinguish them.
When we go farther into the future, the difference between successive points in time will
become insignificant.

